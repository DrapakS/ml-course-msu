{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2. Метод ближайших соседей и решающие деревья."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ФИО: Драпак С. Н.\n",
    "\n",
    "Группа: 317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эксперименты в этой лабораторной работе предлагается проводить на данных соревнования Amazon Employee Access Challenge: https://www.kaggle.com/c/amazon-employee-access-challenge\n",
    "\n",
    "В данной задаче предлагается предсказать, будет ли одобрен запрос сотрудника на получение доступа к тому или иному ресурсу. Все признаки являются категориальными.\n",
    "\n",
    "Для удобства данные можно загрузить по ссылке: https://www.dropbox.com/s/q6fbs1vvhd5kvek/amazon.csv\n",
    "\n",
    "Сразу прочитаем данные и создадим разбиение на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTION</th>\n",
       "      <th>RESOURCE</th>\n",
       "      <th>MGR_ID</th>\n",
       "      <th>ROLE_ROLLUP_1</th>\n",
       "      <th>ROLE_ROLLUP_2</th>\n",
       "      <th>ROLE_DEPTNAME</th>\n",
       "      <th>ROLE_TITLE</th>\n",
       "      <th>ROLE_FAMILY_DESC</th>\n",
       "      <th>ROLE_FAMILY</th>\n",
       "      <th>ROLE_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39353</td>\n",
       "      <td>85475</td>\n",
       "      <td>117961</td>\n",
       "      <td>118300</td>\n",
       "      <td>123472</td>\n",
       "      <td>117905</td>\n",
       "      <td>117906</td>\n",
       "      <td>290919</td>\n",
       "      <td>117908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17183</td>\n",
       "      <td>1540</td>\n",
       "      <td>117961</td>\n",
       "      <td>118343</td>\n",
       "      <td>123125</td>\n",
       "      <td>118536</td>\n",
       "      <td>118536</td>\n",
       "      <td>308574</td>\n",
       "      <td>118539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>36724</td>\n",
       "      <td>14457</td>\n",
       "      <td>118219</td>\n",
       "      <td>118220</td>\n",
       "      <td>117884</td>\n",
       "      <td>117879</td>\n",
       "      <td>267952</td>\n",
       "      <td>19721</td>\n",
       "      <td>117880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>36135</td>\n",
       "      <td>5396</td>\n",
       "      <td>117961</td>\n",
       "      <td>118343</td>\n",
       "      <td>119993</td>\n",
       "      <td>118321</td>\n",
       "      <td>240983</td>\n",
       "      <td>290919</td>\n",
       "      <td>118322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>42680</td>\n",
       "      <td>5905</td>\n",
       "      <td>117929</td>\n",
       "      <td>117930</td>\n",
       "      <td>119569</td>\n",
       "      <td>119323</td>\n",
       "      <td>123932</td>\n",
       "      <td>19793</td>\n",
       "      <td>119325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACTION  RESOURCE  MGR_ID  ROLE_ROLLUP_1  ROLE_ROLLUP_2  ROLE_DEPTNAME  \\\n",
       "0       1     39353   85475         117961         118300         123472   \n",
       "1       1     17183    1540         117961         118343         123125   \n",
       "2       1     36724   14457         118219         118220         117884   \n",
       "3       1     36135    5396         117961         118343         119993   \n",
       "4       1     42680    5905         117929         117930         119569   \n",
       "\n",
       "   ROLE_TITLE  ROLE_FAMILY_DESC  ROLE_FAMILY  ROLE_CODE  \n",
       "0      117905            117906       290919     117908  \n",
       "1      118536            118536       308574     118539  \n",
       "2      117879            267952        19721     117880  \n",
       "3      118321            240983       290919     118322  \n",
       "4      119323            123932        19793     119325  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('amazon.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32769, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9421099209618847"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# доля положительных примеров\n",
    "data.ACTION.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION 2\n",
      "RESOURCE 7518\n",
      "MGR_ID 4243\n",
      "ROLE_ROLLUP_1 128\n",
      "ROLE_ROLLUP_2 177\n",
      "ROLE_DEPTNAME 449\n",
      "ROLE_TITLE 343\n",
      "ROLE_FAMILY_DESC 2358\n",
      "ROLE_FAMILY 67\n",
      "ROLE_CODE 343\n"
     ]
    }
   ],
   "source": [
    "# число значений у признаков\n",
    "for col_name in data.columns:\n",
    "    print col_name, len(data[col_name].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data.iloc[:, 1:]), np.array(data.iloc[:, 0]),\n",
    "                                                    test_size=0.3, random_state=241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1: kNN и категориальные признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Реализуйте три функции расстояния на категориальных признаках, которые обсуждались на втором семинаре. Реализуйте самостоятельно метод k ближайших соседей, который будет уметь работать с этими функциями расстояния (учитите, что он должен возвращать вероятность — отношение объектов первого класса среди соседей к числу соседей). Как вариант, можно реализовать метрики как [user-defined distance](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html), после чего воспользоваться реализацией kNN из sklearn (в этом случае используйте функцию predict_proba).\n",
    "\n",
    "#### Подсчитайте для каждой из метрик качество на тестовой выборке `X_test` при числе соседей $k = 10$. Мера качества — AUC-ROC.\n",
    "\n",
    "Какая функция расстояния оказалась лучшей?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test = np.array([[1, 1, 3], [1, 3, 3], [3, 3, 2], [1, 1, 3]])\n",
    "\n",
    "\n",
    "def f_1d(x, i):\n",
    "    return int(dicts[i].setdefault(x, 0))\n",
    "\n",
    "\n",
    "# Строит матрицу f. Вообще, функцкия немного избыточна,\n",
    "# Но вводить словари я решил не сразу и поскольку она вычисляется быстро,\n",
    "# то строил словарь на ее основе\n",
    "def f(X):\n",
    "    X_t = np.transpose(X)\n",
    "    res = np.ndarray((0, X.shape[0]))\n",
    "    for x in X_t:\n",
    "        tmp_res = np.ndarray(x.shape[0])\n",
    "        for i in xrange(x.shape[0]):\n",
    "            tmp_res[i] = len(np.where(x == x[i])[0])\n",
    "        res = np.vstack((res, tmp_res))\n",
    "   \n",
    "    return np.transpose(res)\n",
    "\n",
    "# Вычисляет словарь f\n",
    "def creat_dict_of_f(X):\n",
    "    global dicts\n",
    "    dicts = np.array([dict() for x in range(X.shape[1])])\n",
    "    f_X = f(X)\n",
    "    for i in xrange(dicts.shape[0]):\n",
    "        for j in xrange(X.shape[0]):\n",
    "            dicts[i].setdefault(X[j, i], f_X[j, i])\n",
    "    #print dicts\n",
    "    return dicts\n",
    "\n",
    "\n",
    "# Словарь p^2\n",
    "def create_dict_of_p2(X):\n",
    "    global p2_dicts\n",
    "    p2_dicts = np.array([dict() for x in range(X.shape[1])])\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(p2_matrix.shape[0]):\n",
    "            p2_dicts[i].setdefault(X[j, i], p2_matrix[j, i])\n",
    "    return p2_dicts\n",
    "\n",
    "            \n",
    "#f_matrix = f(test)\n",
    "\n",
    "# Строит матрицу P\n",
    "def p_compute(X):\n",
    "    res = np.ndarray((0, X.shape[0]))\n",
    "    X_t = np.transpose(X)\n",
    "    k = 0\n",
    "    for x in X_t:\n",
    "        res = np.vstack((res, f_matrix[:, k]/x.shape[0]))\n",
    "        k += 1\n",
    "    return np.transpose(res)\n",
    "\n",
    "\n",
    "# Строит матрицу p\n",
    "def p2_compute(X):\n",
    "    X_t = np.transpose(X)\n",
    "    res = p_compute(X)/(X.shape[0] - 1)\n",
    "    tmp = np.ndarray((0, X.shape[0]))\n",
    "    k = 0\n",
    "    for x in X_t:\n",
    "        tmp = np.vstack((tmp, (f_matrix[:, k] - 1)))\n",
    "        k += 1\n",
    "    tmp = np.transpose(tmp)\n",
    "    return res * tmp\n",
    "    \n",
    "     \n",
    "#p_matrix = p_compute(test)\n",
    "#p2_matrix = p2_compute(test)\n",
    "#print p2_matrix\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Функция считает попарные расстояния вида Di(x, y) = [xi = yj]*log(fi(xi)) * log(fi(yi))\n",
    "\n",
    "def d3_cnb(X, Y, n_nb):\n",
    "    Y_f_matrix = np.ndarray(Y.shape)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y_f_matrix[i, j] = f_1d(Y[i, j], j)\n",
    "    distance_matrix = ((X[:, np.newaxis, :] != Y[np.newaxis, :, :]) * np.log(f_matrix[:, np.newaxis, :] + 1) * np.log(Y_f_matrix[np.newaxis, :, :] + 1)).sum(axis=2)\n",
    "    distance_matrix = (np.transpose(distance_matrix))\n",
    "    return np.argpartition(distance_matrix, n_nb - 1, axis = 1)[:, 0:n_nb]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция создает словарь из сумм по матрице p^2 для всех элементов матрицы X\n",
    "# (сумм из формулы для сглаженного индикатора совпадений)\n",
    "\n",
    "def create_p2_sum(X):\n",
    "    global p2_sum\n",
    "    p2_sum = np.array([dict() for x in range(X.shape[1])])\n",
    "    for i in xrange(X.shape[1]):\n",
    "        for key_2 in dicts[i].keys():\n",
    "            t_sum = 0\n",
    "            for key in dicts[i].keys():\n",
    "                if dicts[i][key] <= dicts[i][key_2]:\n",
    "                    t_sum += p2_dicts[i][key]\n",
    "            p2_sum[i][key_2] = t_sum\n",
    "    return p2_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Считает матрицу попарных расстояний сглаженого индикатора совпадений\n",
    "def smooth_cnb(X, Y, n_nb=10):\n",
    "    #print \"HERE\"\n",
    "    \n",
    "    #print X.shape\n",
    "    #print Y.shape\n",
    "    distance_matrix = ((X[:, np.newaxis, :] != Y[np.newaxis, :, :]).sum(axis=2)).astype(float)\n",
    "    \n",
    "    p2_X = np.ndarray(X.shape)\n",
    "    \n",
    "              \n",
    "    \n",
    "    for i in xrange(X.shape[0]):\n",
    "        for j in xrange(X.shape[1]):\n",
    "                p2_X[i, j] = p2_sum[j][X[i, j]]\n",
    "    \n",
    "    #print p2_X.shape\n",
    "    #print ((X[:, np.newaxis, :] == Y[np.newaxis, :, :]) * p2_X[:, np.newaxis, :]).sum(axis=2)\n",
    "    distance_matrix += ((X[:, np.newaxis, :] == Y[np.newaxis, :, :]) * p2_X[:, np.newaxis, :]).sum(axis=2)\n",
    "           \n",
    "    distance_matrix = np.transpose(distance_matrix)\n",
    "    return np.argpartition(distance_matrix, n_nb - 1, axis = 1)[:, 0:n_nb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Вычисляет матрицу попарных расстояний для индикаторного расстояния\n",
    "\n",
    "def ind_cnb(X, Y, n_nb):\n",
    "    distance_matrix = (X[:, np.newaxis, :] != Y[np.newaxis, :, :]).sum(axis=2)\n",
    "    distance_matrix = np.transpose(distance_matrix)\n",
    "    return np.argpartition(distance_matrix, n_nb - 1, axis = 1)[:, 0:n_nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция взята из задания по практикуму, поэтому немного избыточна.\n",
    "# Она делит тестовую выборку на несколько частей и ищет соседей\n",
    "\n",
    "def divide_and_find_nb(X_train, X_test, n_nb, func_fit, func_find, part=10, return_distance=False):\n",
    "    cn = np.ndarray((0, n_nb), dtype=int)\n",
    "    distance_matrix = np.ndarray((0, n_nb), dtype=float)\n",
    "    if (func_fit != func_find):\n",
    "        func_fit(X_train)\n",
    "        \n",
    "    for p in xrange(part):\n",
    "        if (func_fit == func_find):\n",
    "            cn = np.vstack((cn, func_find(X_train,\n",
    "                                          X_test[p * X_test.shape[0]/part : (p + 1) * X_test.shape[0]/part],\n",
    "                                          n_nb)))\n",
    "        else:\n",
    "            if (return_distance):\n",
    "                sub_distance_matrix, _ = func_find(X_test[p * X_test.shape[0]/part : (p + 1) * X_test.shape[0]/part],\n",
    "                                          n_nb, return_distance=True)\n",
    "                distance_matrix = np.vstack((distance_matrix, sub_distance_matrix))\n",
    "            \n",
    "            cn = np.vstack((cn, func_find(X = X_test[p * X_test.shape[0]/part : (p + 1) * X_test.shape[0]/part],\n",
    "                                          n_neighbors = n_nb, return_distance=False)))\n",
    "            \n",
    "            \n",
    "    if (return_distance):\n",
    "        return distance_matrix, cn\n",
    "    return cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-4ec1d52a3640>:33: SyntaxWarning: name 'f_matrix' is assigned to before global declaration\n",
      "  global f_matrix\n",
      "<ipython-input-19-4ec1d52a3640>:35: SyntaxWarning: name 'global_train' is assigned to before global declaration\n",
      "  global global_train\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial\n",
    "import sys\n",
    "   \n",
    "# Реализация knn\n",
    "def my_knn(X_train, y_train, X_test, metric='ind', n_nb=10):\n",
    "    res = np.ndarray(X_test.shape[0], dtype = float)\n",
    "    nb_matrix = 0\n",
    "    \n",
    "    if (metric == 'ind'):\n",
    "        nb_matrix = divide_and_find_nb(X_train, X_test, n_nb, ind_cnb, ind_cnb)\n",
    "    \n",
    "    if (metric != 'ind'):\n",
    "        creat_dict_of_f(X_train)\n",
    "        \n",
    "    \n",
    "    if (metric == 'smooth_ind'):\n",
    "        global global_train\n",
    "        global_train = X_train\n",
    "        global f_matrix \n",
    "        f_matrix = f(X_train)\n",
    "        global p_matrix\n",
    "        p_matrix = p_compute(X_train)\n",
    "        global p2_matrix\n",
    "        p2_matrix = p2_compute(X_train)\n",
    "        create_dict_of_p2(X_train)\n",
    "        create_p2_sum(X_train)\n",
    "        \n",
    "        #metric_f = smooth_indicator_distance\n",
    "        nb_matrix = divide_and_find_nb(X_train, X_test, n_nb, smooth_cnb, smooth_cnb)\n",
    "    \n",
    "        \n",
    "    if (metric == 'd3'):\n",
    "        global f_matrix \n",
    "        f_matrix = f(X_train)\n",
    "        global global_train\n",
    "        global_train = X_train\n",
    "        nb_matrix = divide_and_find_nb(X_train, X_test, n_nb, d3_cnb, d3_cnb)\n",
    "        \n",
    "    #print nb_matrix.shape\n",
    "    \n",
    "    print \"Preparing computed\"\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    #nb_matrix = closest_nb(X_train, X_test, n_nb, metric_f)\n",
    "    \n",
    "    print \"Distance computed\"\n",
    "    \n",
    "    #print nb_matrix.shape\n",
    "    \n",
    "    for i in xrange(nb_matrix.shape[0]):\n",
    "        classes = y_train[nb_matrix[i]]\n",
    "        cnt = np.bincount(classes)\n",
    "        if len(cnt) == 1:\n",
    "            res[i] = 0\n",
    "            continue        \n",
    "        res[i] = float(cnt[1])/np.sum(cnt)\n",
    "    return res      \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление точности с индикаторным расстоянием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing computed\n",
      "Distance computed\n"
     ]
    }
   ],
   "source": [
    "res = my_knn(X_train, y_train, X_test, metric='ind', n_nb=10)\n",
    "#print accuracy(res, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829499015542\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "print sklearn.metrics.roc_auc_score(y_test, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление точности со сглаженным индикаторным расстоянием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing computed\n",
      "Distance computed\n"
     ]
    }
   ],
   "source": [
    "res_smooth = my_knn(X_train, y_train, X_test, metric='smooth_ind', n_nb=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833094317461\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "print sklearn.metrics.roc_auc_score(y_test, res_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление качества логорифмического расстояния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing computed\n",
      "Distance computed\n",
      "0.821046030503\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "res_3 = my_knn(X_train, y_train, X_test, metric='d3', n_nb=10)\n",
    "print sklearn.metrics.roc_auc_score(y_test, res_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге сглаженное расстояние получилось самым сильным. Сложно сказать, почему именно так, нужно более внимательно изучать признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 (бонус). Подберите лучшее (на тестовой выборке) число соседей $k$ для каждой из функций расстояния. Какое наилучшее качество удалось достичь?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Реализуйте счетчики (http://blogs.technet.com/b/machinelearning/archive/2015/02/17/big-learning-made-easy-with-counts.aspx), которые заменят категориальные признаки на вещественные.\n",
    "\n",
    "А именно, каждый категориальный признак нужно заменить на три: \n",
    "1. Число `counts` объектов в обучающей выборке с таким же значением признака.\n",
    "2. Число `clicks` объектов первого класса ($y = 1$) в обучающей выборке с таким же значением признака.\n",
    "3. Сглаженное отношение двух предыдущих величин: (`clicks` + 1) / (`counts` + 2).\n",
    "\n",
    "Поскольку признаки, содержащие информацию о целевой переменной, могут привести к переобучению, может оказаться полезным сделать *фолдинг*: разбить обучающую выборку на $n$ частей, и для $i$-й части считать `counts` и `clicks` по всем остальным частям. Для тестовой выборки используются счетчики, посчитанный по всей обучающей выборке. Реализуйте и такой вариант. Можно использовать $n = 3$.\n",
    "\n",
    "#### Посчитайте на тесте AUC-ROC метода $k$ ближайших соседей с евклидовой метрикой для выборки, где категориальные признаки заменены на счетчики. Сравните по AUC-ROC два варианта формирования выборки — с фолдингом и без. Не забудьте подобрать наилучшее число соседей $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Считаем счетчики клики и их отношение.\n",
    " Тут хочется прокоментировать один момент. В задании было написано, что для тестовой выборки\n",
    " нужно считать клики по всему трейну, но ведь в такос случие, без доп. обработки мы получим,\n",
    " что значение кликов и каунтов на тесте будет в среднем значительно выше при n = 3.\n",
    " Поскольку никакой речи о нормировки и прочих вещах в задании не было, я выбирал случайные N - N/n объектов из трейна\n",
    " (именно по такому числу мы считаем счетчики для каждого объекта из трейна) и считал их для теста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "\n",
    "\n",
    "def categorial_feat_transorm(x, y, x_test, n_parts = 1):\n",
    "    #print \"HERE\"\n",
    "    counts = np.ndarray(x.shape[0])\n",
    "    clicks = np.ndarray(x.shape[0])\n",
    "    smooth = np.ndarray(x.shape[0])\n",
    "    \n",
    "    counts_test = np.ndarray(x_test.shape[0])\n",
    "    clicks_test = np.ndarray(x_test.shape[0])\n",
    "    smooth_test = np.ndarray(x_test.shape[0])\n",
    "    \n",
    "    k = x.shape[0]/n_parts\n",
    "    for part in range(n_parts):\n",
    "        x_part = x[part * k : (part + 1) * k]\n",
    "        y_part = y[part * k : (part + 1) * k]\n",
    "        \n",
    "        if (n_parts == 1):\n",
    "            rest_ind = range(0, x.shape[0])\n",
    "        else:\n",
    "            rest_ind = range(0, part * k) + range((part + 1) * k , y.shape[0])\n",
    "        #print rest_ind\n",
    "        #print x.shape\n",
    "        x_rest = x[rest_ind]\n",
    "        y_rest = y[rest_ind]\n",
    "        #print x_rest\n",
    "        #print x_part\n",
    "        \n",
    "        for i in range(part * k, (part + 1) * k):\n",
    "            #print x[i].shape\n",
    "            #print x_rest, x_rest == x[i]\n",
    "            #print i, x[i]\n",
    "            counts[i] = len(np.where(x == x[i])[0])\n",
    "            clicks[i] = len(np.where((x_rest == x[i]) & (y_rest == 1))[0])\n",
    "            \n",
    "    smooth = (clicks + 1).astype(float)/(counts + 2).astype(float)\n",
    "    random_indces = range(0, x.shape[0])\n",
    "    if (n_parts != 1):\n",
    "        np.random.shuffle(random_indces)\n",
    "        random_indces = random_indces[:x.shape[0] - k]\n",
    "    #print random_indces\n",
    "    for i in range(x_test.shape[0]):\n",
    "        counts_test[i] = len(np.where(x == x_test[i])[0])\n",
    "        clicks_test[i] = len(np.where((x[random_indces] == x_test[i]) & (y[random_indces] == 1))[0])\n",
    "        smooth_test[i] = float(clicks_test[i] + 1)/float(counts_test[i] + 2)\n",
    "    \n",
    "    return np.transpose(np.vstack((counts, clicks, smooth))), np.transpose(np.vstack((counts_test, clicks_test, smooth_test)))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Считаем клики без фолдов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "New_Train = np.ndarray((X_train.shape[0], 0), dtype=float)\n",
    "New_Test = np.ndarray((X_test.shape[0], 0), dtype=float)\n",
    "for i in xrange(X_train.shape[1]):\n",
    "    #print New_Train.shape, categorial_feat_transorm(x, y_train, 3).shape\n",
    "    t_train, t_test = categorial_feat_transorm(X_train[:, i], y_train, X_test[:, i], 1)\n",
    "    New_Train = np.hstack((New_Train, t_train))\n",
    "    New_Test = np.hstack((New_Test, t_test))\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import normalize\n",
    "#New_Test = normalize(New_Test)\n",
    "#New_Train = normalize(New_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "0.712921090257\n",
      "k = 2\n",
      "0.752472646161\n",
      "k = 3\n",
      "0.775740481957\n",
      "k = 4\n",
      "0.777982378959\n",
      "k = 5\n",
      "0.776867540638\n",
      "k = 6\n",
      "0.784620101343\n",
      "k = 7\n",
      "0.787899830425\n",
      "k = 8\n",
      "0.789534842763\n",
      "k = 9\n",
      "0.78571687869\n",
      "k = 10\n",
      "0.78870008432\n",
      "k = 11\n",
      "0.788626223024\n",
      "k = 12\n",
      "0.791252971524\n",
      "k = 13\n",
      "0.790561532749\n",
      "k = 14\n",
      "0.796091066126\n",
      "k = 15\n",
      "0.794659936123\n",
      "k = 16\n",
      "0.794899311419\n",
      "k = 17\n",
      "0.793385514272\n",
      "k = 18\n",
      "0.791875850483\n",
      "k = 19\n",
      "0.790121599775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for k in range(1, 20):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(New_Train, y_train)\n",
    "    t_res = neigh.predict_proba(New_Test)[:, 1]\n",
    "    #print t_res.shape, y_test.shape\n",
    "    print \"k = \" + str(k)\n",
    "    print roc_auc_score(y_test, t_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С фолдами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "New_Train = np.ndarray((X_train.shape[0], 0), dtype=float)\n",
    "New_Test = np.ndarray((X_test.shape[0], 0), dtype=float)\n",
    "for i in xrange(X_train.shape[1]):\n",
    "    #print New_Train.shape, categorial_feat_transorm(x, y_train, 3).shape\n",
    "    t_train, t_test = categorial_feat_transorm(X_train[:, i], y_train, X_test[:, i], 3)\n",
    "    New_Train = np.hstack((New_Train, t_train))\n",
    "    New_Test = np.hstack((New_Test, t_test))\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import normalize\n",
    "#New_Test = normalize(New_Test)\n",
    "#New_Train = normalize(New_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "0.667906943393\n",
      "k = 2\n",
      "0.702428742719\n",
      "k = 3\n",
      "0.733860498116\n",
      "k = 4\n",
      "0.754097414962\n",
      "k = 5\n",
      "0.760161858674\n",
      "k = 6\n",
      "0.76526583597\n",
      "k = 7\n",
      "0.768760859048\n",
      "k = 8\n",
      "0.771730945762\n",
      "k = 9\n",
      "0.773286525759\n",
      "k = 10\n",
      "0.77024742995\n",
      "k = 11\n",
      "0.771800314279\n",
      "k = 12\n",
      "0.772076350655\n",
      "k = 13\n",
      "0.774218507952\n",
      "k = 14\n",
      "0.7761011623\n",
      "k = 15\n",
      "0.775032330042\n",
      "k = 16\n",
      "0.77916487854\n",
      "k = 17\n",
      "0.780158232102\n",
      "k = 18\n",
      "0.782616411513\n",
      "k = 19\n",
      "0.782001260135\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 20):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(New_Train, y_train)\n",
    "    t_res = neigh.predict_proba(New_Test)[:, 1]\n",
    "    #print t_res.shape, y_test.shape\n",
    "    print \"k = \" + str(k)\n",
    "    print roc_auc_score(y_test, t_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем, что КНН все же лучше пока обучается без фолдов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Добавьте в исходную выборку парные признаки — то есть для каждой пары $f_i$, $f_j$ исходных категориальных признаков добавьте новый категориальный признак $f_{ij}$, значение которого является конкатенацией значений $f_i$ и $f_j$. Посчитайте счетчики для этой выборки, найдите качество метода $k$ ближайших соседей с наилучшим $k$ (с фолдингом и без)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция создает новые параметры по парам. Я решил, что проще и эффективней всего считать хэши от 2х признаков их конкатенацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categorial_feat_concat(X):\n",
    "    new_feat = X\n",
    "    for i in xrange(X.shape[1]):\n",
    "        for j in xrange(i):\n",
    "            tmp = np.ndarray((X.shape[0], 1), dtype=int)\n",
    "            #print (type(tmp[1]))\n",
    "            for k in xrange(X.shape[0]):\n",
    "                #print k, i, j\n",
    "                #print hash((X[k, i], X[k, j]))\n",
    "                tmp[k][0] = hash((X[k, i], X[k, j]))\n",
    "            #print new_feat.shape, tmp.shape\n",
    "            \n",
    "            new_feat = np.hstack((new_feat, tmp))\n",
    "    return new_feat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С конкатенацией, но без фолдов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22938L, 45L)\n"
     ]
    }
   ],
   "source": [
    "New_train_hash = categorial_feat_concat(X_train)\n",
    "New_test_hash = categorial_feat_concat(X_test)\n",
    "print New_train_hash.shape\n",
    "\n",
    "counts_hash_train = np.ndarray((New_train_hash.shape[0], 0), dtype=float)\n",
    "counts_hash_test = np.ndarray((New_test_hash.shape[0], 0), dtype=float)\n",
    "for i in xrange(X_train.shape[1]):\n",
    "    #print New_Train.shape, categorial_feat_transorm(x, y_train, 3).shape\n",
    "    t_train, t_test = categorial_feat_transorm(New_train_hash[:, i], y_train, New_test_hash[:, i], 1)\n",
    "    counts_hash_train = np.hstack((counts_hash_train, t_train))\n",
    "    counts_hash_test = np.hstack((counts_hash_test, t_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "0.712921090257\n",
      "k = 2\n",
      "0.752472646161\n",
      "k = 3\n",
      "0.775740481957\n",
      "k = 4\n",
      "0.777982378959\n",
      "k = 5\n",
      "0.776867540638\n",
      "k = 6\n",
      "0.784620101343\n",
      "k = 7\n",
      "0.787899830425\n",
      "k = 8\n",
      "0.789534842763\n",
      "k = 9\n",
      "0.78571687869\n",
      "k = 10\n",
      "0.78870008432\n",
      "k = 11\n",
      "0.788626223024\n",
      "k = 12\n",
      "0.791252971524\n",
      "k = 13\n",
      "0.790561532749\n",
      "k = 14\n",
      "0.796091066126\n",
      "k = 15\n",
      "0.794659936123\n",
      "k = 16\n",
      "0.794899311419\n",
      "k = 17\n",
      "0.793385514272\n",
      "k = 18\n",
      "0.791875850483\n",
      "k = 19\n",
      "0.790121599775\n"
     ]
    }
   ],
   "source": [
    "#print counts_hash_test.shape\n",
    "for k in range(1, 20):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(counts_hash_train, y_train)\n",
    "    t_res = neigh.predict_proba(counts_hash_test)[:, 1]\n",
    "    #print t_res.shape, y_test.shape\n",
    "    print \"k = \" + str(k)\n",
    "    print roc_auc_score(y_test, t_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С фолдами, и с конкатенацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "New_train_hash = categorial_feat_concat(X_train)\n",
    "New_test_hash = categorial_feat_concat(X_test)\n",
    "\n",
    "\n",
    "counts_hash_train = np.ndarray((New_train_hash.shape[0], 0), dtype=float)\n",
    "counts_hash_test = np.ndarray((New_test_hash.shape[0], 0), dtype=float)\n",
    "for i in xrange(X_train.shape[1]):\n",
    "    #print New_Train.shape, categorial_feat_transorm(x, y_train, 3).shape\n",
    "    t_train, t_test = categorial_feat_transorm(New_train_hash[:, i], y_train, New_test_hash[:, i], 3)\n",
    "    counts_hash_train = np.hstack((counts_hash_train, t_train))\n",
    "    counts_hash_test = np.hstack((counts_hash_test, t_test))\n",
    "    \n",
    "#counts_hash_test = normalize(counts_hash_test)\n",
    "#counts_hash_train = normalize(counts_hash_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "0.669245072859\n",
      "k = 2\n",
      "0.710560134596\n",
      "k = 3\n",
      "0.721695938024\n",
      "k = 4\n",
      "0.735321550031\n",
      "k = 5\n",
      "0.744991539198\n",
      "k = 6\n",
      "0.7565118347\n",
      "k = 7\n",
      "0.759017637574\n",
      "k = 8\n",
      "0.765371955423\n",
      "k = 9\n",
      "0.762708545842\n",
      "k = 10\n",
      "0.770566776722\n",
      "k = 11\n",
      "0.769948570254\n",
      "k = 12\n",
      "0.772768777842\n",
      "k = 13\n",
      "0.773322737562\n",
      "k = 14\n",
      "0.775602913046\n",
      "k = 15\n",
      "0.773233870382\n",
      "k = 16\n",
      "0.7736969861\n",
      "k = 17\n",
      "0.775098104335\n",
      "k = 18\n",
      "0.775729878997\n",
      "k = 19\n",
      "0.775654490156\n",
      "k = 20\n",
      "0.776281861894\n",
      "k = 21\n",
      "0.776492663111\n",
      "k = 22\n",
      "0.77568558019\n",
      "k = 23\n",
      "0.774773815506\n",
      "k = 24\n",
      "0.771463894945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for k in range(1, 25):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(counts_hash_train, y_train)\n",
    "    t_res = neigh.predict_proba(counts_hash_test)[:, 1]\n",
    "    #print t_res.shape, y_test.shape\n",
    "    print \"k = \" + str(k)\n",
    "    print roc_auc_score(y_test, t_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге получилось, что KNN оказался не очень чувствителен к переобучению по кликам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: Решающие деревья и леса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Возьмите из предыдущей части выборку с парными признаками, преобразованную с помощью счетчиков без фолдинга. Настройте решающее дерево, подобрав оптимальные значения параметров `max_depth` и `min_samples_leaf`. Какой наилучший AUC-ROC на контроле удалось получить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем обучать одно дерево без фолдов со счетчиками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_X_train = categorial_feat_concat(X_train)\n",
    "tree_X_test = categorial_feat_concat(X_test)\n",
    "tree_counts_hash_train = np.ndarray((tree_X_train.shape[0], 0), dtype=float)\n",
    "tree_counts_hash_test = np.ndarray((tree_X_test.shape[0], 0), dtype=float)\n",
    "for i in xrange(X_train.shape[1]):\n",
    "    #print New_Train.shape, categorial_feat_transorm(x, y_train, 3).shape\n",
    "    t_train, t_test = categorial_feat_transorm(tree_X_train[:, i], y_train, tree_X_test[:, i], 1)\n",
    "    tree_counts_hash_train = np.hstack((tree_counts_hash_train, t_train))\n",
    "    tree_counts_hash_test = np.hstack((tree_counts_hash_test, t_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 1\n",
      "min_samples_leaf = 1\n",
      "0.66002615876\n",
      "max_depth = 1\n",
      "min_samples_leaf = 2\n",
      "0.66002615876\n",
      "max_depth = 1\n",
      "min_samples_leaf = 5\n",
      "0.66002615876\n",
      "max_depth = 1\n",
      "min_samples_leaf = 10\n",
      "0.66002615876\n",
      "max_depth = 2\n",
      "min_samples_leaf = 1\n",
      "0.675540445439\n",
      "max_depth = 2\n",
      "min_samples_leaf = 2\n",
      "0.675540445439\n",
      "max_depth = 2\n",
      "min_samples_leaf = 5\n",
      "0.675540445439\n",
      "max_depth = 2\n",
      "min_samples_leaf = 10\n",
      "0.675540445439\n",
      "max_depth = 3\n",
      "min_samples_leaf = 1\n",
      "0.70126026061\n",
      "max_depth = 3\n",
      "min_samples_leaf = 2\n",
      "0.701097891557\n",
      "max_depth = 3\n",
      "min_samples_leaf = 5\n",
      "0.701092050943\n",
      "max_depth = 3\n",
      "min_samples_leaf = 10\n",
      "0.701092050943\n",
      "max_depth = 4\n",
      "min_samples_leaf = 1\n",
      "0.669022320848\n",
      "max_depth = 4\n",
      "min_samples_leaf = 2\n",
      "0.668816192121\n",
      "max_depth = 4\n",
      "min_samples_leaf = 5\n",
      "0.668676826099\n",
      "max_depth = 4\n",
      "min_samples_leaf = 10\n",
      "0.668676826099\n",
      "max_depth = 5\n",
      "min_samples_leaf = 1\n",
      "0.673939218803\n",
      "max_depth = 5\n",
      "min_samples_leaf = 2\n",
      "0.673804075993\n",
      "max_depth = 5\n",
      "min_samples_leaf = 5\n",
      "0.673652040333\n",
      "max_depth = 5\n",
      "min_samples_leaf = 10\n",
      "0.684989210141\n",
      "max_depth = 10\n",
      "min_samples_leaf = 1\n",
      "0.663620921545\n",
      "max_depth = 10\n",
      "min_samples_leaf = 2\n",
      "0.663929036367\n",
      "max_depth = 10\n",
      "min_samples_leaf = 5\n",
      "0.675784762792\n",
      "max_depth = 10\n",
      "min_samples_leaf = 10\n",
      "0.68747003316\n",
      "max_depth = 50\n",
      "min_samples_leaf = 1\n",
      "0.655052831493\n",
      "max_depth = 50\n",
      "min_samples_leaf = 2\n",
      "0.654573092492\n",
      "max_depth = 50\n",
      "min_samples_leaf = 5\n",
      "0.677405398236\n",
      "max_depth = 50\n",
      "min_samples_leaf = 10\n",
      "0.696666573217\n",
      "max_depth = 100\n",
      "min_samples_leaf = 1\n",
      "0.651530672026\n",
      "max_depth = 100\n",
      "min_samples_leaf = 2\n",
      "0.655410456747\n",
      "max_depth = 100\n",
      "min_samples_leaf = 5\n",
      "0.679765185775\n",
      "max_depth = 100\n",
      "min_samples_leaf = 10\n",
      "0.700555163785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "depth = [1, 2, 3, 4, 5, 10, 50, 100]\n",
    "ms_leaf = [1, 2, 5, 10]\n",
    "\n",
    "for d in depth:\n",
    "    for ms in ms_leaf:\n",
    "        print \"max_depth = \" + str(d)\n",
    "        print \"min_samples_leaf = \" + str(ms)\n",
    "        clf = DecisionTreeClassifier(max_depth=d, min_samples_leaf=ms)\n",
    "        clf.fit(tree_counts_hash_train, y_train)\n",
    "        tree_res = clf.predict_proba(tree_counts_hash_test)[:, 1]\n",
    "        print roc_auc_score(y_test, tree_res)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Вообще, интересно. При max_depth = 3 вобще не оказалось не важным кол-во объектов в листах. При этом там же и достигается лучший результат 0.701. Хотя при другой глубине результаты другие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Настройте случайный лес, подобрав оптимальное число деревьев `n_estimators`. Какое качество на тестовой выборке он дает?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1\n",
      "0.637915483269\n",
      "n_estimators = 2\n",
      "0.682977702874\n",
      "n_estimators = 3\n",
      "0.719153024791\n",
      "n_estimators = 5\n",
      "0.709746761784\n",
      "n_estimators = 10\n",
      "0.750553780009\n",
      "n_estimators = 15\n",
      "0.745673093857\n",
      "n_estimators = 20\n",
      "0.745201711426\n",
      "n_estimators = 35\n",
      "0.759245331643\n",
      "n_estimators = 50\n",
      "0.760971906829\n",
      "n_estimators = 100\n",
      "0.765144980199\n",
      "n_estimators = 200\n",
      "0.769430283203\n",
      "n_estimators = 500\n",
      "0.768223612468\n",
      "n_estimators = 1000\n",
      "0.770077423172\n",
      "n_estimators = 5000\n",
      "0.769533976556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "n_est = [1, 2, 3, 5, 10, 15, 20, 35, 50, 100, 200, 500, 1000, 5000]\n",
    "for n_estim in n_est:\n",
    "    print \"n_estimators = \" + str(n_estim)\n",
    "    clf = RandomForestClassifier(n_estimators = n_estim)\n",
    "    clf.fit(tree_counts_hash_train, y_train)\n",
    "    random_tree_res = clf.predict_proba(tree_counts_hash_test)[:, 1]\n",
    "    print roc_auc_score(y_test, random_tree_res)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем лучший результат на 1000 деревьев. Хотя он не сильно отличается от результата, например, на 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Возьмите выборку с парными признаками, для которой счетчики посчитаны с фолдингом. Обучите на ней случайный лес, подобрав число деревьев. Какое качество на тестовой выборке он дает? Чем вы можете объяснить изменение результата по сравнению с предыдущим пунктом?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_X_train = categorial_feat_concat(X_train)\n",
    "tree_X_test = categorial_feat_concat(X_test)\n",
    "tree_counts_hash_train_folding = np.ndarray((tree_X_train.shape[0], 0), dtype=float)\n",
    "tree_counts_hash_test_folding = np.ndarray((tree_X_test.shape[0], 0), dtype=float)\n",
    "for i in xrange(X_train.shape[1]):\n",
    "    #print New_Train.shape, categorial_feat_transorm(x, y_train, 3).shape\n",
    "    t_train, t_test = categorial_feat_transorm(tree_X_train[:, i], y_train, tree_X_test[:, i], 3)\n",
    "    tree_counts_hash_train_folding = np.hstack((tree_counts_hash_train_folding, t_train))\n",
    "    tree_counts_hash_test_folding = np.hstack((tree_counts_hash_test_folding, t_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1\n",
      "0.531637075423\n",
      "n_estimators = 2\n",
      "0.577621033684\n",
      "n_estimators = 3\n",
      "0.627162554515\n",
      "n_estimators = 5\n",
      "0.645567765133\n",
      "n_estimators = 10\n",
      "0.692115746941\n",
      "n_estimators = 15\n",
      "0.715047253258\n",
      "n_estimators = 20\n",
      "0.722729996169\n",
      "n_estimators = 35\n",
      "0.766213453035\n",
      "n_estimators = 50\n",
      "0.771360740726\n",
      "n_estimators = 100\n",
      "0.791176863839\n",
      "n_estimators = 200\n",
      "0.787900549269\n",
      "n_estimators = 500\n",
      "0.792868125778\n",
      "n_estimators = 1000\n",
      "0.795329270423\n",
      "n_estimators = 5000\n",
      "0.793171118832\n"
     ]
    }
   ],
   "source": [
    "n_est = [1, 2, 3, 5, 10, 15, 20, 35, 50, 100, 200, 500, 1000, 5000]\n",
    "\n",
    "\n",
    "#tree_counts_hash_test_folding = normalize(tree_counts_hash_test_folding)\n",
    "#tree_counts_hash_train_folding = normalize(tree_counts_hash_train_folding)\n",
    "for n_estim in n_est:\n",
    "    print \"n_estimators = \" + str(n_estim)\n",
    "    clf = RandomForestClassifier(n_estimators = n_estim)\n",
    "    clf.fit(tree_counts_hash_train_folding, y_train)\n",
    "    random_tree_res = clf.predict_proba(tree_counts_hash_test_folding)[:, 1]\n",
    "    print roc_auc_score(y_test, random_tree_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Вообще, результат заметно улучшился. Вообще, есть идея, что KNN считает расстояние, и вообще говоря, там выкидывай не выкидывай на фолдах, особо ничего не поменяется. А когда строятся деревья, они могут делать деления как раз по кликам. И вот тогда внесение этой лишний информации из теста может быть фатальным и будет происходить переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
